# Survivorship Bias

Survivorship bias is the logical error of concentrating on the people or things that made it past a selection process and overlooking those that did not. It leads to false conclusions because failures are invisible.

## The Principle

> "The dead don't write books." — Nassim Nicholas Taleb

You study successful startups to learn what makes startups succeed. Problem: you're only looking at the winners. The failures — which may have done many of the same things — aren't in the dataset. The "winning formula" you extracted might be noise.

## The Classic Example

During World War II, the military studied returning bombers to decide where to add armor. The planes had bullet holes clustered on the wings and fuselage, so the obvious conclusion was to reinforce those areas.

Statistician Abraham Wald pointed out the error: the planes that made it back were the ones that *survived* hits to those areas. The planes hit in the engines and cockpit never returned. The military needed to armor the places with *no* bullet holes — those were the fatal spots.

## Where It Shows Up

### Business Strategy

**"Successful companies do X, so we should do X."**

For every company that succeeded by pivoting, there's a graveyard of companies that pivoted and died. For every founder who dropped out of college and built a billion-dollar company, there are thousands who dropped out and failed. You don't hear their stories.

**The correction:** Study failures as carefully as successes. Ask "what did companies that failed do?" not just "what did companies that succeeded do?"

### Product Development

**"Our users love this feature."**

Your current users are survivors — people who stayed despite everything else. The people who left because of a missing feature aren't giving feedback anymore. User feedback is inherently biased toward survivors.

**The correction:** Study churn. Exit interviews. People who signed up but never activated. The silent quitters hold more insight than the vocal fans.

### Hiring

**"Our interview process works — look at how good our team is."**

You're evaluating the people you hired, not the people you rejected. Some of those rejected candidates might have been great. You'll never know.

**The correction:** Track false negatives where possible. Did candidates you rejected go on to succeed elsewhere? If so, your process might be filtering out good people.

### Investment

**"This fund has a 15-year track record of beating the market."**

Of the 1,000 funds that existed 15 years ago, most have been shut down or merged (because they underperformed). The ones still reporting are the survivors. The "average" fund looks better than it is because the failures disappear from the data.

**The correction:** Look at inception-cohort data. How did all funds started in Year X perform, including the ones that closed?

### Personal Development

**"These 10 habits made me successful."**

Successful people attribute their success to their habits. But plenty of unsuccessful people have the same habits. The habits might be correlated with success without causing it — or they might be table stakes that are necessary but not sufficient.

**The correction:** Ask what unsuccessful people with the same habits are doing differently. The difference is likely the actual cause.

## How to Correct for It

### 1. Ask: "What am I not seeing?"

Every dataset has a selection filter. Identify it. What got filtered out? What happened to the filtered-out cases?

### 2. Seek Negative Evidence

Deliberately look for counterexamples. For every success story that supports your theory, look for a failure story that contradicts it.

### 3. Study Bases Rates

Before asking "why did X succeed?" ask "what percentage of things like X succeed?" If the base rate is 90%, the success isn't remarkable. If it's 1%, it is.

### 4. Include Failure Data

When making decisions, give equal weight to failure cases. Build a "failure library" alongside your success case studies.

### 5. Beware of Advice

Successful people's advice is survivorship-biased by definition. They did thing X and succeeded, but they don't know if X caused the success or if they would have succeeded anyway.

## The Meta-Lesson

Survivorship bias is itself a lesson in [signal vs. noise](/wiki/concepts/signal-vs-noise.html). The visible data (survivors) is noise. The invisible data (non-survivors) is signal. The hardest part of analysis isn't processing data — it's knowing which data you're missing.

---

*See also: [Cognitive Biases](/wiki/concepts/cognitive-biases.html) | [Signal vs. Noise](/wiki/concepts/signal-vs-noise.html) | [First Principles Thinking](/wiki/concepts/first-principles.html)*
